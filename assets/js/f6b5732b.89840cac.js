"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8543],{4137:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return g}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},h="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),h=p(n),m=o,g=h["".concat(l,".").concat(m)]||h[m]||u[m]||i;return n?a.createElement(g,s(s({ref:t},c),{},{components:n})):a.createElement(g,s({ref:t},c))}));function g(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,s=new Array(i);s[0]=m;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r[h]="string"==typeof e?e:o,s[1]=r;for(var p=2;p<i;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},444:function(e,t,n){n.r(t),n.d(t,{contentTitle:function(){return l},default:function(){return m},frontMatter:function(){return r},metadata:function(){return p},toc:function(){return c}});var a=n(7462),o=n(3366),i=(n(7294),n(4137)),s=["components"],r={title:"Apache Pinot\u2122 0.11 - Inserts from SQL",author:"Mark Needham",author_title:"Mark Needham",author_url:"https://www.linkedin.com/in/lakshmanan-velusamy-a778a517/",author_image_url:"https://www.datocms-assets.com/75153/1661544338-mark-needham.png",description:"Explore the INSERT INTO clause, which makes ingesting batch data into Pinot as easy as writing a SQL query.",keywords:["Apache Pinot","Apache Pinot 0.11.0","Insert Into"],tags:["Pinot","Data","Analytics","User-Facing Analytics","Insert"]},l=void 0,p={permalink:"/blog/2022/11/17/Apache Pinot-Inserts-from-SQL",editUrl:"https://github.com/apache/pinot-site/edit/dev/website/blog/2022-11-17-Apache Pinot-Inserts-from-SQL.md",source:"@site/blog/2022-11-17-Apache Pinot-Inserts-from-SQL.md",title:"Apache Pinot\u2122 0.11 - Inserts from SQL",description:"Explore the INSERT INTO clause, which makes ingesting batch data into Pinot as easy as writing a SQL query.",date:"2022-11-17T00:00:00.000Z",formattedDate:"November 17, 2022",tags:[{label:"Pinot",permalink:"/blog/tags/pinot"},{label:"Data",permalink:"/blog/tags/data"},{label:"Analytics",permalink:"/blog/tags/analytics"},{label:"User-Facing Analytics",permalink:"/blog/tags/user-facing-analytics"},{label:"Insert",permalink:"/blog/tags/insert"}],readingTime:3.775,truncated:!1,prevItem:{title:"Apache Pinot\u2122 0.11 - Timestamp Indexes",permalink:"/blog/2022/11/22/Apache-Pinot-Timestamp-Indexes"},nextItem:{title:"Apache Pinot\u2122 0.11 - How do I see my indexes?",permalink:"/blog/2022/11/08/Apache Pinot-How-do-I-see-my-indexes"}},c=[{value:"Batch importing: The Job Specification",id:"batch-importing-the-job-specification",children:[]},{value:"Batch Importing with SQL",id:"batch-importing-with-sql",children:[]},{value:"Summary",id:"summary",children:[]}],h={toc:c},u="wrapper";function m(e){var t=e.components,n=(0,o.Z)(e,s);return(0,i.kt)(u,(0,a.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"The Apache Pinot community recently released version ",(0,i.kt)("a",{parentName:"p",href:"https://medium.com/apache-pinot-developer-blog/apache-pinot-0-11-released-d564684df5d4"},"0.11.0"),", which has lots of goodies for you to play with. This is the second in a series of blog posts showing off some of the new features in this release."),(0,i.kt)("p",null,"In this post, we\u2019re going to explore the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.pinot.apache.org/basics/data-import/from-query-console"},"INSERT INTO clause"),", which makes ingesting batch data into Pinot as easy as writing a SQL query."),(0,i.kt)("h2",{id:"batch-importing-the-job-specification"},"Batch importing: The Job Specification"),(0,i.kt)("p",null,"The power of this new clause is only fully appreciated if we look at what we had to do before it existed.\xa0"),(0,i.kt)("p",null,"In the ",(0,i.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=1EMBx1XeI9o"},"Batch Import JSON from Amazon S3 into Apache Pinot | StarTree Recipes")," video (and ",(0,i.kt)("a",{parentName:"p",href:"https://dev.startree.ai/docs/pinot/recipes/ingest-csv-files-from-s3"},"accompanying developer guide"),"), we showed how to ingest data into Pinot from an S3 bucket."),(0,i.kt)("p",null,"The contents of that bucket are shown in the screenshot below:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1668701275-image4.png",alt:"Sample data ingested into Apache Pinot from a S3 bucket",title:"Sample data ingested into Apache Pinot from a S3 bucket"})),(0,i.kt)("p",null,"Let\u2019s quickly recap the steps that we had to do to import those files into Pinot. We have a table called events, which has the following schema:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1668701353-image1.png",alt:"Events schema table",title:"Events schema table"})),(0,i.kt)("p",null,"We first created a job specification file, which contains a description of our import job. The job file is shown below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"executionFrameworkSpec:\n  name: 'standalone'\n  segmentGenerationJobRunnerClassName: 'org.apache.pinot.plugin.ingestion.batch.standalone.SegmentGenerationJobRunner'\n  segmentTarPushJobRunnerClassName: 'org.apache.pinot.plugin.ingestion.batch.standalone.SegmentTarPushJobRunner'\n  segmentUriPushJobRunnerClassName: 'org.apache.pinot.plugin.ingestion.batch.standalone.SegmentUriPushJobRunner'\njobType: SegmentCreationAndTarPush\ninputDirURI: 's3://marks-st-cloud-bucket/events/'\nincludeFileNamePattern: 'glob:**/*.json'\noutputDirURI: '/data'\noverwriteOutput: true\npinotFSSpecs:\n  - scheme: s3\n    className: org.apache.pinot.plugin.filesystem.S3PinotFS\n    configs:\n      region: 'eu-west-2'\n  - scheme: file\n    className: org.apache.pinot.spi.filesystem.LocalPinotFS\nrecordReaderSpec:\n  dataFormat: 'json'\n  className: 'org.apache.pinot.plugin.inputformat.json.JSONRecordReader'\ntableSpec:\n  tableName: 'events'\npinotClusterSpecs:\n  - controllerURI: 'http://${PINOT_CONTROLLER}:9000'\n")),(0,i.kt)("p",null,"At a high level, this file describes a batch import job that will ingest files from the S3 bucket at s3://marks-st-cloud-bucket/events/ where the files match the glob:","*","*","/","*",".json pattern."),(0,i.kt)("p",null,"We can import the data by running the following command from the terminal:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"docker run \\\n  --network ingest-json-files-s3 \\\n  -v $PWD/config:/config \\\n  -e AWS_ACCESS_KEY_ID=AKIARCOCT6DWLUB7F77Z \\\n  -e AWS_SECRET_ACCESS_KEY=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 \\\n  apachepinot/pinot:0.11.0 LaunchDataIngestionJob \\\n  -jobSpecFile /config/job-spec.yml \\\n  -values PINOT_CONTROLLER=pinot-controller\n")),(0,i.kt)("p",null,"And don\u2019t worry, those credentials have already been deleted; I find it easier to understand what values go where if we use real values.\xa0"),(0,i.kt)("p",null,"Once we\u2019ve run this command, if we go to the Pinot UI\xa0at ",(0,i.kt)("a",{parentName:"p",href:"http://localhost:9000/"},"http://localhost:9000")," and click through to the events table from the Query Console menu, we\u2019ll see that the records have been imported, as shown in the screenshot below:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1668701512-image3.png",alt:"Sample imported records shown in the Apache Pinot Query Console menu",title:"Sample imported records shown in the Apache Pinot Query Console menu"})),(0,i.kt)("p",null,"This approach works, and we may still prefer to use it when we need fine-grained control over the ingestion parameters, but it is a bit heavyweight for your everyday data import!"),(0,i.kt)("h2",{id:"batch-importing-with-sql"},"Batch Importing with SQL"),(0,i.kt)("p",null,"Now let\u2019s do the same thing in SQL."),(0,i.kt)("p",null,"There are some prerequisites to using the SQL approach, so let\u2019s go through those now, so you don\u2019t end up with a bunch of exceptions when you try this out!\xa0"),(0,i.kt)("p",null,"First of all, you must have a ",(0,i.kt)("a",{parentName:"p",href:"https://docs.pinot.apache.org/basics/components/minion"},"Minion")," in the Pinot cluster, as this is the component that will do the data import."),(0,i.kt)("p",null,"You\u2019ll also need to include the following in your table config:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'"task": {\n  "taskTypeConfigsMap": { "SegmentGenerationAndPushTask": {} }\n}\n')),(0,i.kt)("p",null,"As long as you\u2019ve done those two things, we\u2019re ready to write our import query! A query that imports JSON files from my S3 bucket is shown below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sql"},"INSERT INTO events\nFROM FILE 's3://marks-st-cloud-bucket/events/'\nOPTION(\n  taskName=events-task,\n  includeFileNamePattern=glob:**/*.json,\n  input.fs.className=org.apache.pinot.plugin.filesystem.S3PinotFS,\n  input.fs.prop.accessKey=AKIARCOCT6DWLUB7F77Z,\n  input.fs.prop.secretKey=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,\n  input.fs.prop.region=eu-west-2\n);\n")),(0,i.kt)("p",null,"If we run this query, we\u2019ll see the following output:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1668701654-image5.png",alt:"Sample events_OFFLINE query result",title:"Sample events_OFFLINE query result"})),(0,i.kt)("p",null,"We can check on the state of the ingestion job via the Swagger REST API. If we navigate to ",(0,i.kt)("a",{parentName:"p",href:"http://localhost:9000/help#/Task/getTaskState"},"http://localhost:9000/help#/Task/getTaskState"),", paste Task","_","SegmentGenerationAndPushTask","_","events-task as our task name, and then click Execute, we\u2019ll see the following:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1668701727-image2.png",alt:"Checking the state of an ingestion job screen",title:"Checking the state of an ingestion job screen"})),(0,i.kt)("p",null,"If we see the state COMPLETED, this means the data has been ingested, which we can check by going back to the Query console and clicking on the events table."),(0,i.kt)("h2",{id:"summary"},"Summary"),(0,i.kt)("p",null,"I have to say that batch ingestion of data into Apache Pinot has always felt a bit clunky, but with this new clause, it\u2019s super easy, and it\u2019s gonna save us all a bunch of time."),(0,i.kt)("p",null,"Also, anything that means I\u2019m not writing YAML files has got to be a good thing!"),(0,i.kt)("p",null,"So give it a try and let us know how you get on. If you have any questions about this feature, feel free to join us on ",(0,i.kt)("a",{parentName:"p",href:"https://stree.ai/slack"},"Slack"),", where we\u2019ll be happy to help you out."))}m.isMDXComponent=!0}}]);