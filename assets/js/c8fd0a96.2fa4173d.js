"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3711],{4137:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return d}});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},s=Object.keys(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,s=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),m=c(a),h=o,d=m["".concat(l,".").concat(h)]||m[h]||u[h]||s;return a?n.createElement(d,r(r({ref:t},p),{},{components:a})):n.createElement(d,r({ref:t},p))}));function d(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var s=a.length,r=new Array(s);r[0]=h;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[m]="string"==typeof e?e:o,r[1]=i;for(var c=2;c<s;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},2156:function(e,t,a){a.r(t),a.d(t,{contentTitle:function(){return l},default:function(){return h},frontMatter:function(){return i},metadata:function(){return c},toc:function(){return p}});var n=a(7462),o=a(3366),s=(a(7294),a(4137)),r=["components"],i={title:"Real-Time Mastodon Usage with Apache Kafka, Apache Pinot, and Streamlit",author:"Mark Needham",author_title:"Developer Advocate",author_url:"https://pinot.apache.org/",author_image_url:"https://pinot.apache.org/authors/pinot_team.jpg",description:"The blog post discusses analyzing user activity and server popularity on Mastodon using Kafka Connect, Parquet, Seaborn, and DuckDB. It explores the potential of using Apache Pinot for real-time data streaming and creating a dashboard. The post provides instructions on ingesting Apache Avro messages into Pinot, creating a Pinot table, and querying the data.",keywords:["Apache Pinot","blog post","analyzing user activity","server popularity","Mastodon","Kafka Connect","Parquet","Seaborn","DuckDB","realtime data streaming","dashboard","Apache Avro messages"],tags:["Pinot","Data","Analytics","User-Facing Analytics","blog post","analyzing user activity","server popularity","Mastodon","Kafka Connect","Parquet","Seaborn","DuckDB","potential","Apache Pinot","realtime data streaming","dashboard","instructions","ingesting","Apache Avro messages","Pinot table","querying data"]},l=void 0,c={permalink:"/blog/2023/06/01/real-time-mastodon-usage-with-apache-kafka-apache-pinot-and-streamlit",editUrl:"https://github.com/apache/pinot-site/edit/dev/website/blog/2023-06-01-real-time-mastodon-usage-with-apache-kafka-apache-pinot-and-streamlit.md",source:"@site/blog/2023-06-01-real-time-mastodon-usage-with-apache-kafka-apache-pinot-and-streamlit.md",title:"Real-Time Mastodon Usage with Apache Kafka, Apache Pinot, and Streamlit",description:"The blog post discusses analyzing user activity and server popularity on Mastodon using Kafka Connect, Parquet, Seaborn, and DuckDB. It explores the potential of using Apache Pinot for real-time data streaming and creating a dashboard. The post provides instructions on ingesting Apache Avro messages into Pinot, creating a Pinot table, and querying the data.",date:"2023-06-01T00:00:00.000Z",formattedDate:"June 1, 2023",tags:[{label:"Pinot",permalink:"/blog/tags/pinot"},{label:"Data",permalink:"/blog/tags/data"},{label:"Analytics",permalink:"/blog/tags/analytics"},{label:"User-Facing Analytics",permalink:"/blog/tags/user-facing-analytics"},{label:"blog post",permalink:"/blog/tags/blog-post"},{label:"analyzing user activity",permalink:"/blog/tags/analyzing-user-activity"},{label:"server popularity",permalink:"/blog/tags/server-popularity"},{label:"Mastodon",permalink:"/blog/tags/mastodon"},{label:"Kafka Connect",permalink:"/blog/tags/kafka-connect"},{label:"Parquet",permalink:"/blog/tags/parquet"},{label:"Seaborn",permalink:"/blog/tags/seaborn"},{label:"DuckDB",permalink:"/blog/tags/duck-db"},{label:"potential",permalink:"/blog/tags/potential"},{label:"Apache Pinot",permalink:"/blog/tags/apache-pinot"},{label:"realtime data streaming",permalink:"/blog/tags/realtime-data-streaming"},{label:"dashboard",permalink:"/blog/tags/dashboard"},{label:"instructions",permalink:"/blog/tags/instructions"},{label:"ingesting",permalink:"/blog/tags/ingesting"},{label:"Apache Avro messages",permalink:"/blog/tags/apache-avro-messages"},{label:"Pinot table",permalink:"/blog/tags/pinot-table"},{label:"querying data",permalink:"/blog/tags/querying-data"}],readingTime:6.39,truncated:!1,prevItem:{title:"Star-Tree Index in Apache Pinot - Part 3 - Understanding the Impact in Real Customer Scenarios",permalink:"/blog/2023/07/12/star-tree-index-in-apache-pinot-part-3-understanding-the-impact-in-real-customer"},nextItem:{title:"How to Ingest Streaming Data from Kafka to Apache Pinot\u2122",permalink:"/blog/2023/05/30/how-to-ingest-streaming-data-from-kafka-to-apache-pinot"}},p=[{value:"The Existing Solution: Kafka Connect, Parquet, Seaborn and DuckDB\xa0",id:"the-existing-solution-kafka-connect-parquet-seaborn-and-duckdb",children:[]},{value:"Going Real-Time with Apache Pinot\u2122",id:"going-real-time-with-apache-pinot",children:[]},{value:"Setup",id:"setup",children:[]},{value:"Pinot Schema and Table",id:"pinot-schema-and-table",children:[]},{value:"Ingest Data into Kafka",id:"ingest-data-into-kafka",children:[]},{value:"Query Pinot",id:"query-pinot",children:[]},{value:"Streamlit",id:"streamlit",children:[]},{value:"Summary",id:"summary",children:[]}],m={toc:p},u="wrapper";function h(e){var t=e.components,a=(0,o.Z)(e,r);return(0,s.kt)(u,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("p",null,"I recently came across a fascinating blog post written by Simon Aubury that shows ",(0,s.kt)("a",{parentName:"p",href:"https://simonaubury.com/posts/202302_mastodon_duckdb/"},"how to analyze user activity, server popularity, and language usage on Mastodon"),", a decentralized social networking platform that has become quite popular in the last six months.\xa0"),(0,s.kt)("h2",{id:"the-existing-solution-kafka-connect-parquet-seaborn-and-duckdb"},"The Existing Solution: Kafka Connect, Parquet, Seaborn and DuckDB\xa0"),(0,s.kt)("p",null,"To start, Simon wrote a listener to collect the messages, which he then published into Apache Kafka\xae. He then wrote a Kafka Connect configuration that consumes messages from Kafka and flushes them after every 1,000 messages into Apache Parquet files stored in an Amazon S3 bucket.\xa0"),(0,s.kt)("p",null,"Finally, he queried those Parquet files using DuckDB and created some charts using the Seaborn library, as reflected in the architecture diagram below:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685637607-image1.png",alt:"Flowchart of data collection to data processing",title:"Flowchart of data collection to data processing"})),(0,s.kt)("p",null,"Fig: ",(0,s.kt)("a",{parentName:"p",href:"https://simonaubury.com/posts/202302_mastodon_duckdb/"},"Data Collection Architecture")),(0,s.kt)("p",null,"The awesome visualizations that Simon created make me wonder whether we can change what happens downstream of Kafka to make our queries even more real-time. Let\u2019s find out!"),(0,s.kt)("h2",{id:"going-real-time-with-apache-pinot"},"Going Real-Time with Apache Pinot\u2122"),(0,s.kt)("p",null,"Now ",(0,s.kt)("a",{parentName:"p",href:"https://startree.ai/resources/what-is-apache-pinot"},"Apache Pinot")," comes into the picture. Instead of using Kafka Connect to batch Mastodon toots into groups of 1,000 messages to generate Parquet files, we can stream the data immediately and directly, toot-by-toot into Pinot and then build a real-time dashboard using Streamlit:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685637507-image4.png",alt:"Data collection in Mastodon, followed by processing in Apache Kafka, Apache Pinot, and Streamlit",title:"Data collection in Mastodon, followed by processing in Apache Kafka, Apache Pinot, and Streamlit"})),(0,s.kt)("h2",{id:"setup"},"Setup"),(0,s.kt)("p",null,"To follow along, first clone my fork of Simon\u2019s GitHub repository:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"git clone git@github.com:mneedham/mastodon-stream.git\ncd mastodon-stream\n")),(0,s.kt)("p",null,"Then launch all of the components using Docker Compose:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"docker-compose up\n")),(0,s.kt)("h2",{id:"pinot-schema-and-table"},"Pinot Schema and Table"),(0,s.kt)("p",null,"Similar to what Simon did with DuckDB, we\u2019ll ingest the Mastodon events into a table. Pinot tables have a schema that\u2019s defined in a schema file.\xa0"),(0,s.kt)("p",null,"To come up with a schema file, we need to know the structure of the ingested events. For example:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "m_id": 110146691030544274,\n  "created_at": 1680705124,\n  "created_at_str": "2023 04 05 15:32:04",\n  "app": "",\n  "url": "https://mastodon.social/@Xingcat/110146690810165414",\n  "base_url": "https://techhub.social",\n  "language": "en",\n  "favourites": 0,\n  "username": "Xingcat",\n  "bot": false,\n  "tags": 0,\n  "characters": 196,\n  "words": 36,\n  "mastodon_text": "Another, \u201cI don\u2019t know what this is yet,\u201d paintings. Many, many layers that look like distressed metal or some sort of rock crosscut. Liking it so far, need to figure out what it\u2019ll wind up being."\n}\n')),(0,s.kt)("p",null,"Mapping these fields directly to columns is easiest and will result in a schema file that looks like this:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "schemaName":"mastodon",\n  "dimensionFieldSpecs":[\n    {"name":"m_id","dataType":"LONG"},\n    {"name":"created_at_str","dataType":"STRING"},\n    {"name":"app","dataType":"STRING"},\n    {"name":"url","dataType":"STRING"},\n    {"name":"base_url","dataType":"STRING"},\n    {"name":"language","dataType":"STRING"},\n    {"name":"username","dataType":"STRING"},\n    {"name":"bot","dataType":"BOOLEAN"},    \n    {"name":"mastodon_text","dataType":"STRING"}\n  ],\n  "metricFieldSpecs":[\n    {"name":"favourites","dataType":"INT"},\n    {"name":"words","dataType":"INT"},\n    {"name":"characters","dataType":"INT"},\n    {"name":"tags","dataType":"INT"}\n  ],\n  "dateTimeFieldSpecs":[\n    {\n      "name":"created_at",\n      "dataType":"LONG",\n      "format":"1:MILLISECONDS:EPOCH",\n      "granularity":"1:MILLISECONDS"\n    }\n  ]\n}\n')),(0,s.kt)("p",null,"Next up: our table config, shown below:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "tableName": "mastodon",\n    "tableType": "REALTIME",\n    "segmentsConfig": {\n      "timeColumnName": "created_at",\n      "timeType": "MILLISECONDS",\n      "schemaName": "mastodon",\n      "replicasPerPartition": "1"\n    },\n    "tenants": {},\n    "tableIndexConfig": {\n      "loadMode": "MMAP",\n      "streamConfigs": {\n        "streamType": "kafka",\n        "stream.kafka.consumer.type": "lowLevel",\n        "stream.kafka.topic.name": "mastodon-topic",\n        "stream.kafka.decoder.class.name": "org.apache.pinot.plugin.inputformat.avro.confluent.KafkaConfluentSchemaRegistryAvroMessageDecoder",\n        "stream.kafka.consumer.factory.class.name": "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",\n        "stream.kafka.decoder.prop.format": "AVRO",\n        "stream.kafka.decoder.prop.schema.registry.rest.url": "http://schema-registry:8081",\n        "stream.kafka.decoder.prop.schema.registry.schema.name": "mastodon-topic-value",\n        "stream.kafka.broker.list": "broker:9093",\n        "stream.kafka.consumer.prop.auto.offset.reset": "smallest"\n      }\n    },\n    "metadata": {\n      "customConfigs": {}\n    },\n    "routing": {\n      "instanceSelectorType": "strictReplicaGroup"\n    }\n}\n')),(0,s.kt)("p",null,"The following configs represent the most important ones for ingesting Apache Avro\u2122 messages into Pinot:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json"},'"stream.kafka.decoder.class.name": "org.apache.pinot.plugin.inputformat.avro.confluent.KafkaConfluentSchemaRegistryAvroMessageDecoder",\n"stream.kafka.decoder.prop.format": "AVRO",\n"stream.kafka.decoder.prop.schema.registry.rest.url": "http://schema-registry:8081",\n"stream.kafka.decoder.prop.schema.registry.schema.name": "mastodon-topic-value",\n')),(0,s.kt)("p",null,"The KafkaConfluentSchemaRegistryAvroMessageDecoder decoder calls the Schema Registry with the schema name to get back the schema that it will use to decode messages."),(0,s.kt)("p",null,"We can create the Pinot table by running the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},'docker run \\\n   --network mastodon \\\n   -v $PWD/pinot:/config \\\n   apachepinot/pinot:0.12.0-arm64 AddTable \\\n     -schemaFile /config/schema.json \\\n     -tableConfigFile /config/table.json \\\n     -controllerHost "pinot-controller" \\\n    -exec\n')),(0,s.kt)("p",null,"We can then navigate to the table page of the Pinot UI:\xa0"),(0,s.kt)("p",null,"http://localhost:9000/#/tenants/table/mastodon","_","REALTIME"),(0,s.kt)("p",null,"Here, we\u2019ll see the following:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685637837-image6.png",alt:"Apache Pinot table config and schema",title:"Apache Pinot table config and schema"})),(0,s.kt)("h2",{id:"ingest-data-into-kafka"},"Ingest Data into Kafka"),(0,s.kt)("p",null,"Now, we need to start ingesting data into Kafka. Simon created a script that accomplishes this for us, so we just need to indicate which Mastodon servers to query."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"python mastodonlisten.py --baseURL https://data-folks.masto.host \\\n  --public --enableKafka --quiet\npython mastodonlisten.py --baseURL https://fosstodon.org/ \\\n  --public --enableKafka --quiet\npython mastodonlisten.py --baseURL https://mstdn.social/ \\\n  --public --enableKafka --quiet\n")),(0,s.kt)("p",null,"We can then check the ingestion of messages with the ",(0,s.kt)("a",{parentName:"p",href:"https://docs.confluent.io/platform/current/clients/kafkacat-usage.html"},"kcat")," command line tool:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"kcat -C -b localhost:9092 -t mastodon-topic \\\n  -s value=avro -r http://localhost:8081 -e\n")),(0,s.kt)("h2",{id:"query-pinot"},"Query Pinot"),(0,s.kt)("p",null,"Now, let\u2019s go to the Pinot UI to see what data we\u2019ve got to play with:"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"http://localhost:9000/"},"http://localhost:9000")),(0,s.kt)("p",null,"We\u2019ll see the following preview of the data in the mastodon table:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685637772-image5.png",alt:"SQL Editor, query response stats, and query result in Apache Pinot",title:"SQL Editor, query response stats, and query result in Apache Pinot"})),(0,s.kt)("p",null,"We can then write a query to find the number of messages posted in the last five minutes:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-sql"},'select count(*) as "Num toots"\n, count(distinct(username)) as "Num users"\n, count(distinct(url)) as "Num urls"\nfrom mastodon\nwhere created_at*1000 > ago(\'PT1M\')\norder by 1 DESC;\n')),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685637909-image8.png",alt:"Query results for toots, users, and urls",title:"Query results for toots, users, and urls"})),(0,s.kt)("p",null,"We can also query Pinot via the Python client, which we can install by running the following:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"pip install pinotdb\n")),(0,s.kt)("p",null,"Once we\u2019ve done that, let\u2019s open the Python REPL and run the following code:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'from pinotdb import connect\nimport pandas as pd\n\nconn = connect(host=\'localhost\', port=8099, path=\'/query/sql\', scheme=\'http\')\n\ncurs = conn.cursor()\n\nst.header("Daily Mastodon Usage")\nquery = """\nselect count(*) as "Num toots"\n, count(distinct(username)) as "Num users"\n, count(distinct(url)) as "Num urls"\nfrom mastodon\nwhere created_at*1000 > ago(\'PT1M\')\norder by 1 DESC;\n"""\ncurs.execute(query)\n\ndf = pd.DataFrame(curs, columns=[item[0] for item in curs.description])\n')),(0,s.kt)("p",null,"This produces the resulting DataFrame:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"   Num toots  Num users  Num urls\n0        552        173       192\n")),(0,s.kt)("h2",{id:"streamlit"},"Streamlit"),(0,s.kt)("p",null,"Next, we\u2019ll create a Streamlit dashboard to package up these queries. We\u2019ll visualize the results using Plotly, which you can install using:"),(0,s.kt)("p",null,"pip install streamlit plotly"),(0,s.kt)("p",null,"I\u2019ve created a Streamlit app in the file ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/mneedham/mastodon-stream/blob/main/app.py"},"app.py"),", which you can find in the GitHub repository. Let\u2019s have a look at the kinds of visualizations that we can generate.\xa0"),(0,s.kt)("p",null,"First, we\u2019ll create metrics to show the number of toots, users, and URLs in the last ",(0,s.kt)("em",{parentName:"p"},"n")," minutes. ",(0,s.kt)("em",{parentName:"p"},"n")," will be configurable from the app as shown in the screenshot below:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685637876-image7.png",alt:"Chart of real-time Mastodon usage",title:"Chart of real-time Mastodon usage"})),(0,s.kt)("p",null,"From the screenshot, we can identify mastodon.cloud as the most active server, though it produces only 1,800 messages in 10 minutes or three messages per second. The values in green indicate the change in values compared to the previous 10 minutes."),(0,s.kt)("p",null,"We can also create a chart showing the number of messages per minute for the last 10 minutes:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685637945-image9.png",alt:"Time of day Mastodon usage",title:"Time of day Mastodon usage"})),(0,s.kt)("p",null,"Based on this chart, we can see that we\u2019re creating anywhere from 200\u2013900 messages per second. Part of the reason lies in the fact that the Mastodon servers sometimes disconnect our listener, and at the moment, I have to manually reconnect."),(0,s.kt)("p",null,"Finally, we can look at the toot length by language:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685637644-image2.png",alt:"Chart of toot length by language usage",title:"Chart of toot length by language usage"})),(0,s.kt)("p",null,"We see much bigger ranges here than Simon saw in his analysis. He saw a maximum length of 200 characters, whereas we see some messages of up to 4,200 characters.\xa0"),(0,s.kt)("h2",{id:"summary"},"Summary"),(0,s.kt)("p",null,"We hope you enjoyed following along as we explored this fun use case for ",(0,s.kt)("a",{parentName:"p",href:"https://startree.ai/resources/what-is-real-time-analytics"},"real-time analytics"),". As you can see, even though we\u2019re pulling the data from many of the popular Mastodon servers, it\u2019s still not all that much data!"),(0,s.kt)("p",null,"Give the code a try and let us know how it goes. If you have any questions, feel free to ",(0,s.kt)("a",{parentName:"p",href:"https://stree.ai/slack"},"join us on Slack"),", where we\u2019ll gladly do our best to help you out."))}h.isMDXComponent=!0}}]);