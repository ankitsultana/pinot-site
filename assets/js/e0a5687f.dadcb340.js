"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1149],{4137:function(e,t,a){a.d(t,{Zo:function(){return c},kt:function(){return h}});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(a),u=o,h=m["".concat(l,".").concat(u)]||m[u]||d[u]||i;return a?n.createElement(h,r(r({ref:t},c),{},{components:a})):n.createElement(h,r({ref:t},c))}));function h(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,r=new Array(i);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:o,r[1]=s;for(var p=2;p<i;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},114:function(e,t,a){a.r(t),a.d(t,{contentTitle:function(){return l},default:function(){return u},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return c}});var n=a(7462),o=a(3366),i=(a(7294),a(4137)),r=["components"],s={title:"How to Ingest Streaming Data from Kafka to Apache Pinot\u2122",author:"Barkha Herman",author_title:"Developer Advocate",author_url:"https://pinot.apache.org/",author_image_url:"https://pinot.apache.org/authors/pinot_team.jpg",description:"The blog post explains how to use Apache Kafka topics in Apache Pinot to ingest streaming data, with step-by-step instructions provided for installation and setup. It focuses on ingesting Wikipedia events into Kafka and connecting it to Pinot to create a real-time table. The post highlights Pinot's capabilities in ingesting and transforming JSON data into OLAP tables and encourages reader engagement through the community Slack.",keywords:["Apache Pinot","Apache Kafka","streaming data","JSON data"],tags:["Pinot","Data","Analytics","User-Facing Analytics","kafka","streaming","json"]},l=void 0,p={permalink:"/blog/2023/05/30/how-to-ingest-streaming-data-from-kafka-to-apache-pinot",editUrl:"https://github.com/apache/pinot-site/edit/dev/website/blog/2023-05-30-how-to-ingest-streaming-data-from-kafka-to-apache-pinot.md",source:"@site/blog/2023-05-30-how-to-ingest-streaming-data-from-kafka-to-apache-pinot.md",title:"How to Ingest Streaming Data from Kafka to Apache Pinot\u2122",description:"The blog post explains how to use Apache Kafka topics in Apache Pinot to ingest streaming data, with step-by-step instructions provided for installation and setup. It focuses on ingesting Wikipedia events into Kafka and connecting it to Pinot to create a real-time table. The post highlights Pinot's capabilities in ingesting and transforming JSON data into OLAP tables and encourages reader engagement through the community Slack.",date:"2023-05-30T00:00:00.000Z",formattedDate:"May 30, 2023",tags:[{label:"Pinot",permalink:"/blog/tags/pinot"},{label:"Data",permalink:"/blog/tags/data"},{label:"Analytics",permalink:"/blog/tags/analytics"},{label:"User-Facing Analytics",permalink:"/blog/tags/user-facing-analytics"},{label:"kafka",permalink:"/blog/tags/kafka"},{label:"streaming",permalink:"/blog/tags/streaming"},{label:"json",permalink:"/blog/tags/json"}],readingTime:8.935,truncated:!1,prevItem:{title:"Real-Time Mastodon Usage with Apache Kafka, Apache Pinot, and Streamlit",permalink:"/blog/2023/06/01/real-time-mastodon-usage-with-apache-kafka-apache-pinot-and-streamlit"},nextItem:{title:"Change Data Capture with Apache Pinot - How Does It Work?",permalink:"/blog/2023/05/23/change-data-capture-with-apache-pinot-how-does-it-work"}},c=[{value:"The obligatory \u201cWhat is Apache Pinot and StarTree?\u201d section",id:"the-obligatory-what-is-apache-pinot-and-startree-section",children:[]},{value:"How to install Kafka alongside Pinot\xa0",id:"how-to-install-kafka-alongside-pinot",children:[{value:"Prerequisite",id:"prerequisite",children:[]},{value:"Step 1: Install Kafka on your Pinot Docker image",id:"step-1-install-kafka-on-your-pinot-docker-image",children:[]},{value:"Step 2: Install Kafka on the Docker container",id:"step-2-install-kafka-on-the-docker-container",children:[]},{value:"Step 3: Ingest data into Kafka",id:"step-3-ingest-data-into-kafka",children:[]},{value:"Step 4: Connect Kafka to Pinot",id:"step-4-connect-kafka-to-pinot",children:[]}]},{value:"Conclusion",id:"conclusion",children:[]}],m={toc:c},d="wrapper";function u(e){var t=e.components,a=(0,o.Z)(e,r);return(0,i.kt)(d,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"We previously walked through getting started with ",(0,i.kt)("a",{parentName:"p",href:"https://startree.ai/resources/what-is-apache-pinot"},"Apache Pinot\u2122")," using batch data, and now we will learn how to ingest streaming data using Apache Kafka\xae topics.\xa0"),(0,i.kt)("p",null,"As the story goes, Apache Pinot was created at LinkedIn to provide a platform that could ingest a high number of incoming events (kafka) and provide \u201cfresh\u201d (sub second) analytics to a large number (20+ million) of users, fast (sub second latency). So, really, consuming events is part of the reason why Pinot was created."),(0,i.kt)("h3",{id:"the-obligatory-what-is-apache-pinot-and-startree-section"},"The obligatory \u201cWhat is Apache Pinot and StarTree?\u201d section"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://docs.pinot.apache.org/"},"Pinot")," is a real-time, distributed, open source, and free-to-use OLAP datastore, purpose-built to provide ultra low-latency analytics at extremely high throughput. It is open source and free to use."),(0,i.kt)("p",null,"How does StarTree come in? StarTree offers a ",(0,i.kt)("a",{parentName:"p",href:"https://startree.ai/saas-signup"},"fully managed version of the Apache Pinot real-time analytics system")," , plus other tools around it that you can try for free. The system includes\xa0 ",(0,i.kt)("a",{parentName:"p",href:"https://dev.startree.ai/docs/startree-enterprise-edition/startree-dataset-manager/"},"StarTree Dataset Manager")," and ",(0,i.kt)("a",{parentName:"p",href:"https://dev.startree.ai/docs/procedures/get-started-with-thirdeye/"},"StarTree ThirdEye"),", a UI based data ingestion tool, and a real-time anomaly detection and root cause analysis tool, respectively."),(0,i.kt)("h2",{id:"how-to-install-kafka-alongside-pinot"},"How to install Kafka alongside Pinot\xa0"),(0,i.kt)("h3",{id:"prerequisite"},"Prerequisite"),(0,i.kt)("p",null,"Complete the steps outlined in the ",(0,i.kt)("a",{parentName:"p",href:"https://startree.ai/blog/apache-pinot-tutorial-for-getting-started-a-step-by-step-guide"},"introduction to Apache Pinot"),".\xa0"),(0,i.kt)("h3",{id:"step-1-install-kafka-on-your-pinot-docker-image"},"Step 1: Install Kafka on your Pinot Docker image"),(0,i.kt)("p",null,"Make sure you have completed the first article in the series."),(0,i.kt)("p",null,"We will be installing Apache Kafka onto our already existing Pinot docker image. To start the Docker image, run the following command:"),(0,i.kt)("p",null,"docker run -it --entrypoint /bin/bash -p 9000:9000 apachepinot/pinot:0.12.0"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462020-image1.png",alt:"PowerShell 7.3.4 docker run Apache Pinot",title:"PowerShell 7.3.4 docker run Apache Pinot"})),(0,i.kt)("p",null,"We want to override the ENTRYPOINT and run Bash script within the Docker image. If you already have a container running, you can skip this step. I tend to tear down containers after use, so in my case, I created a brand new container."),(0,i.kt)("p",null,"Now, start each of the components one at a time like we did in the previous session:"),(0,i.kt)("p",null,"bin/pinot-admin.sh StartZookeeper &"),(0,i.kt)("p",null,"bin/pinot-admin.sh StartController &"),(0,i.kt)("p",null,"bin/pinot-admin.sh StartBroker &"),(0,i.kt)("p",null,"bin/pinot-admin.sh StartServer &"),(0,i.kt)("p",null,"Run each of the commands one at a time. The & allows you to continue using the same Bash shell session. If you like, you can create different shells for each service:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Get the container ID by running docker ps"),(0,i.kt)("li",{parentName:"ol"},"Run ",(0,i.kt)("inlineCode",{parentName:"li"},"docker exec -it DOCKER_CONTAINER_ID bash")," where DOCKER_CONTAINER_ID is the ID received from step 1."),(0,i.kt)("li",{parentName:"ol"},"Run the pinot-admin.sh command to start the desired service")),(0,i.kt)("p",null,"It should look like this:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462274-image7.png",alt:"Docker with container ID, Image, Command, and Created",title:"Docker with container ID, Image, Command, and Created"})),(0,i.kt)("p",null,"You can now browse to ",(0,i.kt)("a",{parentName:"p",href:"http://localhost:9000/#/zookeeper"},"http://localhost:9000/#/zookeeper")," to see the running cluster:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462203-image5.png",alt:"Empty Zookeeper Browser",title:"Empty Zookeeper Browser"})),(0,i.kt)("h3",{id:"step-2-install-kafka-on-the-docker-container"},"Step 2: Install Kafka on the Docker container"),(0,i.kt)("p",null,"Next, let's install Kafka. We will be installing Kafka on the existing docker container. For this step, download the TAR file, extract the contents, and start Kafka."),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Apache Kafka is an open source software platform that provides a unified, high-throughput, low-latency platform for handling real-time data feeds.")),(0,i.kt)("p",null,"Use the following command to download the Kafka image:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"cd ..\ncurl https://downloads.apache.org/kafka/3.4.0/kafka_2.12-3.4.0.tgz --output kafka.tgz --output kafka.tgz\n")),(0,i.kt)("p",null,"It should look this:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462322-image8.png",alt:"Code with Apache Pinot speed results",title:"Code with Apache Pinot speed results"})),(0,i.kt)("p",null,"Note that we\u2019ve changed the directory to keep the Kafka folder separate from the Pinot folder."),(0,i.kt)("p",null,"Now, let\u2019s expand the downloaded TAR file, rename the folder for convenience, and delete the downloaded file:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"tar -xvf kafka.tgz\nmv kafka_2.12-3.4.0 kafka\nrm -rf kafka.tgz\n")),(0,i.kt)("p",null,"It should look like this:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462061-image2.png",alt:"Code with Apache Kafka",title:"Code with Apache Kafka"})),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462480-image12.png",alt:"Code with kafka version",title:"Code with kafka version"}),"  "),(0,i.kt)("p",null,"Now, Kafka and Pinot reside locally on our Docker container with Pinot up and running. Let\u2019s run the Kafka service. Kafka will use the existing ZooKeeper for configuration management."),(0,i.kt)("p",null,"Use the following command to run Kafka:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"cd kafka\n./bin/kafka-server-start.sh config/server.properties\n")),(0,i.kt)("p",null,"It should look like this:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462400-image10.png",alt:"Code with cd kafka",title:"Code with cd kafka"})),(0,i.kt)("p",null,"To verify that Kafka is running, let\u2019s look at our ZooKeeper configs by browsing to ",(0,i.kt)("a",{parentName:"p",href:"http://localhost:9000/#/zookeeper"},"http://localhost:9000/#/zookeeper:")),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462099-image3.png",alt:"Zookeeper Browser",title:"Zookeeper Browser"})),(0,i.kt)("p",null,"You may have to refresh the page and find many more configuration items appear thanexpectedt. These are Kafka configurations.\xa0"),(0,i.kt)("h3",{id:"step-3-ingest-data-into-kafka"},"Step 3: Ingest data into Kafka"),(0,i.kt)("p",null,"In this step, we will ingest data into Kafka. We will be using Wikipedia events since they are easily accessible. We will use a node script to ingest the Wikipedia events, then add them to a Kafka Topic."),(0,i.kt)("p",null,"Let\u2019s first create some folders like this:"),(0,i.kt)("p",null,"cd /opt"),(0,i.kt)("p",null,"mkdir realtime"),(0,i.kt)("p",null,"cd realtime"),(0,i.kt)("p",null,"mkdir events"),(0,i.kt)("p",null,"It should look like this:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462150-image4.png",alt:"Code with realtime",title:"Code with realtime"})),(0,i.kt)("p",null,"You may have to start a new PowerShell window and connect to Docker for this. Now, let\u2019s install Node.js and any dependencies we might need for the event consumption script:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"curl -fsSL https://deb.nodesource.com/setup_14.x | bash -\napt install nodejs\n")),(0,i.kt)("p",null,"Node.js takes a few minutes to install. Next, we will create a script to consume the events called wikievents.js. Cut and paste the following code to this file:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-javascript"},'var EventSource = require("eventsource");\nvar fs = require("fs");\nvar path = require("path");\nconst { Kafka } = require("kafkajs");\n\nvar url = "https://stream.wikimedia.org/v2/stream/recentchange";\n\nconst kafka = new Kafka({\n clientId: "wikievents",\n brokers: ["localhost:9092"],\n});\n\nconst producer = kafka.producer();\n\nasync function start() {\n await producer.connect();\n startEvents();\n}\n\nfunction startEvents() {\n console.log(`Connecting to EventStreams at ${url}`);\n var eventSource = new EventSource(url);\n\n eventSource.onopen = function () {\n console.log("--- Opened connection.");\n };\n\n eventSource.onerror = function (event) {\n console.error("--- Encountered error", event);\n };\n\n eventSource.onmessage = async function (event) {\n const data = JSON.parse(event.data);\n const eventPath = path.join(__dirname, "./events", data.wiki);\n fs.existsSync(eventPath) || fs.mkdirSync(eventPath);\n fs.writeFileSync(path.join(eventPath, data.meta.id + ".json"), event.data);\n await producer.send({\n topic: "wikipedia-events",\n messages: [\n {\n key: data.meta.id,\n value: event.data,\n },\n ],\n });\n };\n}\n\nstart();\n')),(0,i.kt)("p",null,"You can use vi to create the file and save it. You can also use Docker Desktop to edit the file."),(0,i.kt)("p",null,"To install the two modules referenced in the file above, kafkajs and eventsource, run the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"npm i eventsource kafkajs\n")),(0,i.kt)("p",null,"Let\u2019s run the program. This will result in the download of many files, so I recommend running the program for just a few minutes. You can stop the run by using Ctrl-C."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"node wikievents.js\n")),(0,i.kt)("p",null,"Use Ctrl-C to stop the program. Navigate to the events folder to see some new folders created with the various language events downloaded from Wikipedia."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462366-image9.png",alt:"Wikievents node in code",title:"Wikievents node in code"})),(0,i.kt)("p",null,"Navigate to the enwiki folder and review some of the downloaded JSON files."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462441-image11.png",alt:"Code with realtime wikievents",title:"Code with realtime wikievents"})),(0,i.kt)("p",null,"At http://localhost:9000/#/zookeeper, you can find the Kafka topic by locating the ZooKeeper config and expanding config > topics. You may have to refresh your browser."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462510-image13.png",alt:"Zookeeper browser in Apache Pinot topics",title:"Zookeeper browser in Apache Pinot topics"})),(0,i.kt)("p",null,"Here, you should see the wikipedia-events topic that we created using the Node.js script. So far, so good."),(0,i.kt)("h3",{id:"step-4-connect-kafka-to-pinot"},"Step 4: Connect Kafka to Pinot"),(0,i.kt)("p",null,"With Kafka installed and configured to receive events, we can connect it to Pinot.\xa0"),(0,i.kt)("p",null,"To create a real-time table in Pinot that can consume the Kafka topic, create a schema and a configuration table. The schema configuration is very much like the schema that we created for our batch example. You can use vi to create a file named realtime.schema.json and cut and paste the content below."),(0,i.kt)("p",null,"Here\u2019s the JSON for the wikievents schema:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n "schemaName": "wikievents",\n "dimensionFieldSpecs": [\n {\n "name": "id",\n "dataType": "STRING"\n },\n {\n "name": "wiki",\n "dataType": "STRING"\n },\n {\n "name": "user",\n "dataType": "STRING"\n },\n {\n "name": "title",\n "dataType": "STRING"\n },\n {\n "name": "comment",\n "dataType": "STRING"\n },\n {\n "name": "stream",\n "dataType": "STRING"\n },\n {\n "name": "domain",\n "dataType": "STRING"\n },\n {\n "name": "topic",\n "dataType": "STRING"\n },\n {\n "name": "type",\n "dataType": "STRING"\n },\n {\n "name": "metaJson",\n "dataType": "STRING"\n }\n ],\n "dateTimeFieldSpecs": [\n {\n "name": "timestamp",\n "dataType": "LONG",\n "format": "1:MILLISECONDS:EPOCH",\n "granularity": "1:MILLISECONDS"\n }\n ]\n}\n')),(0,i.kt)("p",null,"Creating the table config file is where the magic happens. Use vi (or your favorite editor) to create realtime.tableconfig.json and cut and paste the following content:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n "tableName": "wikievents_REALTIME",\n "tableType": "REALTIME",\n "segmentsConfig": {\n "timeColumnName": "timestamp",\n "schemaName": "wikievents",\n "replication": "1",\n "replicasPerPartition": "1"\n },\n "tenants": {\n "broker": "DefaultTenant",\n "server": "DefaultTenant",\n "tagOverrideConfig": {}\n },\n "tableIndexConfig": {\n "invertedIndexColumns": [],\n "rangeIndexColumns": [],\n "autoGeneratedInvertedIndex": false,\n "createInvertedIndexDuringSegmentGeneration": false,\n "sortedColumn": [],\n "bloomFilterColumns": [],\n "loadMode": "MMAP",\n "streamConfigs": {\n "streamType": "kafka",\n "stream.kafka.topic.name": "wikipedia-events",\n "stream.kafka.broker.list": "localhost:9092",\n "stream.kafka.consumer.type": "lowlevel",\n "stream.kafka.consumer.prop.auto.offset.reset": "smallest",\n "stream.kafka.consumer.factory.class.name": "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",\n "stream.kafka.decoder.class.name": "org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder",\n "realtime.segment.flush.threshold.rows": "0",\n "realtime.segment.flush.threshold.time": "24h",\n "realtime.segment.flush.segment.size": "100M"\n },\n "noDictionaryColumns": [],\n "onHeapDictionaryColumns": [],\n "varLengthDictionaryColumns": [],\n "enableDefaultStarTree": false,\n "enableDynamicStarTreeCreation": false,\n "aggregateMetrics": false,\n "nullHandlingEnabled": false\n },\n "metadata": {},\n "quota": {},\n "routing": {},\n "query": {},\n "ingestionConfig": {\n "transformConfigs": [\n {\n "columnName": "metaJson",\n "transformFunction": "JSONFORMAT(meta)"\n },\n {\n "columnName": "id",\n "transformFunction": "JSONPATH(metaJson, \'$.id\')"\n },\n {\n "columnName": "stream",\n "transformFunction": "JSONPATH(metaJson, \'$.stream\')"\n },\n {\n "columnName": "domain",\n "transformFunction": "JSONPATH(metaJson, \'$.domain\')"\n },\n {\n "columnName": "topic",\n "transformFunction": "JSONPATH(metaJson, \'$.topic\')"\n }\n ]\n },\n "isDimTable": false\n}\n')),(0,i.kt)("p",null,"Notice the section called streamConfigs, where we define the source as a Kafka stream, located at localhost:9092, and consume the topic wikipedia-events. That\u2019s all it takes to consume a Kafka Topic into Pinot."),(0,i.kt)("p",null,"Don\u2019t believe me? Give it a try!"),(0,i.kt)("p",null,"Create the table by running the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"/opt/pinot/bin/pinot-admin.sh AddTable -schemaFile /opt/realtime/realtime.schema.json -tableConfigFile /opt/realtime/realtime.tableconfig.json -exec\n")),(0,i.kt)("p",null,"Now, browse to the following location ",(0,i.kt)("a",{parentName:"p",href:"http://localhost:9000/#/tables"},"http://localhost:9000/#/tables,")," and you should see the newly created table. However, where\u2019s the real-time data, you say?"),(0,i.kt)("p",null,"Run the node wikievents.js command, then query the newly created wikievents table to see the totalDocs increase in real time:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://www.datocms-assets.com/75153/1685462248-image6.png",alt:"Apache Pinot query console",title:"Apache Pinot query console"})),(0,i.kt)("p",null,"To avoid running out of space on your computer, make sure to stop the wikievents.js script when you\u2019re done :-D"),(0,i.kt)("h2",{id:"conclusion"},"Conclusion"),(0,i.kt)("p",null,"Congratulations! Using only the table config, we simultaneously consumed Kafka topics directly into Pinot tables and queried events. We also transformed JSON to map to the Pinot table. In the transformConfigs portion of the Pinot table config file, we consumed the nested block meta into a field called metaJson. In the subsequent steps, we referenced the metaJson field with jsonPath to extract fields such as id, stream, domain, and topic.\xa0"),(0,i.kt)("p",null,"Not only does Pinot support easy ingestion from Kafka topics, but it also provides a robust way to transform JSON to OLAP tables.\xa0"),(0,i.kt)("p",null,"In summary, we have:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Installed and run Kafka"),(0,i.kt)("li",{parentName:"ul"},"Consumed events from Wikipedia into Kafka"),(0,i.kt)("li",{parentName:"ul"},"Created a real-time table schema and a table in Pinot"),(0,i.kt)("li",{parentName:"ul"},"Streamed events from Wikipedia into Pinot tables via Kafka topics"),(0,i.kt)("li",{parentName:"ul"},"Run multiple queries"),(0,i.kt)("li",{parentName:"ul"},"Performed JSON transformations")),(0,i.kt)("p",null,"In some upcoming blog posts, we will explore more advanced topics, such as indexes and transformations, not to mention real-time anomaly detection with ",(0,i.kt)("a",{parentName:"p",href:"https://dev.startree.ai/docs/procedures/get-started-with-thirdeye/"},"ThirdEye"),"."),(0,i.kt)("p",null,"In the meantime, run more queries, load more data, and don\u2019t forget to ",(0,i.kt)("a",{parentName:"p",href:"https://dev.startree.ai/slack-invite"},"join the community Slack for support")," if you get stuck or would like to request a topic for me to write about\u2014you know where to find us!"))}u.isMDXComponent=!0}}]);